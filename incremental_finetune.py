# incremental_finetune.py
# =========================================================
# å¢é‡å¾®è°ƒå•ç«™æ¨¡å‹  (ä¸ WeightedL1 + prev_load é€»è¾‘ä¸€è‡´)
# Updated for vpp_meter.csv with dual outputs (energy + power)
# =========================================================
import os, warnings, joblib, holidays, pandas as pd, numpy as np
from sklearn.metrics import mean_absolute_percentage_error
import torch, torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau
warnings.filterwarnings("ignore")

# ---------------- CONFIG ----------------
STATION_ID  = 1851144626925211648      # â† ç›®æ ‡åœºç«™
NEW_CSV     = 'vpp_meter.csv'   # â† æ–°å¢æ•°æ®ä½¿ç”¨ vpp_meter.csv
EPOCHS      = 30                 # å¢åŠ è®­ç»ƒè½®æ•°
BATCH_SIZE  = 64                 # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
LR_INCR     = 1e-4               # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç²¾ç»†è°ƒæ•´
# ---------------------------------------

ROOT        = 'output_pytorch'
MODEL_DIR   = f'{ROOT}/model_{STATION_ID}'
os.makedirs(MODEL_DIR, exist_ok=True)

PAST, FUT   = 96*5, 96*7
DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ---------- is_peak ----------
def make_is_peak(ts):
    h,mi = ts.dt.hour, ts.dt.minute
    return (((h>8)|((h==8)&(mi>=30))) & ((h<17)|((h==17)&(mi<=30)))).astype(int)

# ---------- è¯»å–å°¾å·´ç¼“å­˜ (ä¸Šæ¬¡ PAST+FUT) ----------
TAIL_PATH = f'{MODEL_DIR}/tail_cache_{STATION_ID}.csv'
old_tail  = pd.read_csv(TAIL_PATH, parse_dates=['ts']) if os.path.exists(TAIL_PATH) else pd.DataFrame()
if not old_tail.empty and 'ts' in old_tail.columns:
    old_tail = old_tail.rename(columns={'ts': 'energy_date'})

# ---------- è¯»å–æ–°å¢ CSV ----------
df_new = pd.read_csv(NEW_CSV, parse_dates=['ts'])
if 'station_ref_id' in df_new.columns:
    df_new = df_new[df_new['station_ref_id'] == STATION_ID]
if df_new.empty:
    raise ValueError('æ–°å¢ CSV ä¸­æ— è¯¥åœºç«™æ•°æ®')

# é‡å‘½åå­—æ®µä»¥ä¿æŒä»£ç å…¼å®¹æ€§
df_new = df_new.rename(columns={
    'ts': 'energy_date',
    'forward_total_active_energy': 'load_discharge_delta'
})

# åˆå¹¶ã€æŒ‰æ—¶é—´æ’åº
df = pd.concat([old_tail, df_new], ignore_index=True).sort_values('energy_date').reset_index(drop=True)

# å¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µï¼Œè®¾ç½®é»˜è®¤å€¼
optional_fields = {
    'temp': 25.0,           # é»˜è®¤æ¸©åº¦25åº¦
    'humidity': 60.0,       # é»˜è®¤æ¹¿åº¦60%
    'windSpeed': 5.0,       # é»˜è®¤é£é€Ÿ5m/s
    'is_work': None,        # å°†æ ¹æ®æ—¥æœŸè®¡ç®—
    'is_peak': None,        # å°†æ ¹æ®æ—¶é—´è®¡ç®—
    'code': 999             # é»˜è®¤ä»£ç 
}

for field, default_value in optional_fields.items():
    if field not in df.columns:
        if field in ['is_work', 'is_peak']:
            df[field] = 0  # ä¸´æ—¶è®¾ç½®ï¼Œåé¢ä¼šé‡æ–°è®¡ç®—
        else:
            df[field] = default_value
        print(f"WARNING: Field '{field}' missing, set default value: {default_value}")

# ---------- enrich (å¿…é¡»ä¸æ€»æ¨¡å‹ä¸€è‡´) ----------
cn_holidays = holidays.country_holidays('CN')
def enrich(d):
    d['hour']    = d['energy_date'].dt.hour
    d['minute']  = d['energy_date'].dt.minute
    d['weekday'] = d['energy_date'].dt.weekday
    d['month']   = d['energy_date'].dt.month
    d['day']     = d['energy_date'].dt.day

    # å¤©æ°”ç‰©ç†ç‰¹å¾ï¼ˆå¦‚æœæœ‰å¤©æ°”æ•°æ®ï¼‰
    if 'temp' in d.columns and 'humidity' in d.columns:
        d['dew_point']  = d['temp'] - (100-d['humidity'])/5
        d['feels_like'] = d['temp'] + 0.33*d['humidity'] - 4
        for k in [1,24]:
            d[f'temp_diff{k}'] = d['temp'].diff(k)
    else:
        # å¦‚æœæ²¡æœ‰å¤©æ°”æ•°æ®ï¼Œåˆ›å»ºè™šæ‹Ÿç‰¹å¾
        d['dew_point'] = 20.0
        d['feels_like'] = 25.0
        d['temp_diff1'] = 0.0
        d['temp_diff24'] = 0.0

    d['sin_hour'] = np.sin(2*np.pi*(d['hour']+d['minute']/60)/24)
    d['cos_hour'] = np.cos(2*np.pi*(d['hour']+d['minute']/60)/24)
    d['sin_wday'] = np.sin(2*np.pi*d['weekday']/7)
    d['cos_wday'] = np.cos(2*np.pi*d['weekday']/7)

    d['is_holiday'] = d['energy_date'].isin(cn_holidays).astype(int)
    d['is_work']    = ((d['weekday']<5)&(~d['energy_date'].isin(cn_holidays))).astype(int)
    d['is_peak']    = make_is_peak(d['energy_date']).astype(int)
    return d
df = enrich(df)

# ---------- å¢å¼ºç‰¹å¾å·¥ç¨‹ ----------
# åŸºç¡€æ»åç‰¹å¾
for lag in [1,2,4,8,12,24,48,96,192,288]:  # å¢åŠ æ›´é•¿æœŸæ»å
    df[f'load_lag{lag}'] = df['load_discharge_delta'].shift(lag)

# æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
for w in [4,8,12,24,48,96,192]:  # å¢åŠ æ›´é•¿çª—å£
    df[f'load_ma{w}']  = df['load_discharge_delta'].rolling(w,1).mean()
    df[f'load_std{w}'] = df['load_discharge_delta'].rolling(w,1).std()
    df[f'load_min{w}'] = df['load_discharge_delta'].rolling(w,1).min()
    df[f'load_max{w}'] = df['load_discharge_delta'].rolling(w,1).max()
    df[f'load_q25{w}'] = df['load_discharge_delta'].rolling(w,1).quantile(0.25)
    df[f'load_q75{w}'] = df['load_discharge_delta'].rolling(w,1).quantile(0.75)

# å‘¨æœŸæ€§ç‰¹å¾å¢å¼º
df['load_lag_week'] = df['load_discharge_delta'].shift(96*7)  # åŒä¸€å‘¨æœŸ
df['load_lag_day'] = df['load_discharge_delta'].shift(96)     # åŒä¸€æ—¶åˆ»æ˜¨å¤©
df['load_ma_week'] = df['load_discharge_delta'].rolling(96*7,1).mean()

# å·®åˆ†ç‰¹å¾
df['load_diff1'] = df['load_discharge_delta'].diff(1)
df['load_diff24'] = df['load_discharge_delta'].diff(24)
df['load_diff96'] = df['load_discharge_delta'].diff(96)

# æ¯”ç‡ç‰¹å¾
df['load_ratio_ma24'] = df['load_discharge_delta'] / (df['load_ma24'] + 1e-6)
df['load_ratio_ma96'] = df['load_discharge_delta'] / (df['load_ma96'] + 1e-6)

# æ—¶é—´äº¤äº’ç‰¹å¾
df['hour_load_interaction'] = df['hour'] * df['load_discharge_delta']
df['weekday_load_interaction'] = df['weekday'] * df['load_discharge_delta']
df['is_peak_load_interaction'] = df['is_peak'] * df['load_discharge_delta']

df['prev_load'] = df['load_discharge_delta'].shift(1)

df = df.fillna(method='ffill').fillna(method='bfill').dropna()

# æ¸…ç†æ— ç©·å¤§å’Œå¼‚å¸¸å€¼
print("INFO: Cleaning infinite values and outliers...")
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if col in df.columns:
        # æ›¿æ¢æ— ç©·å¤§å€¼
        df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        # å¡«å…… NaN
        df[col] = df[col].fillna(df[col].median() if not df[col].isna().all() else 0)
        # å¤„ç†æç«¯å¼‚å¸¸å€¼ (è¶…è¿‡99.9%åˆ†ä½æ•°çš„å€¼)
        if df[col].std() > 0:
            upper_bound = df[col].quantile(0.999)
            lower_bound = df[col].quantile(0.001)
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# ---------- è½½å…¥ç‰¹å¾åˆ— & Scaler ----------
enc_cols = joblib.load(f'{ROOT}/enc_cols.pkl')
dec_cols = joblib.load(f'{ROOT}/dec_cols.pkl')
sc_enc   = joblib.load(f'{ROOT}/scaler_enc.pkl')
sc_dec   = joblib.load(f'{ROOT}/scaler_dec.pkl')
sc_y_energy = joblib.load(f'{ROOT}/scaler_y_energy.pkl')
sc_y_power  = joblib.load(f'{ROOT}/scaler_y_power.pkl')

# è¡¥å…¨ç¼ºå¤±åˆ—
for col in set(enc_cols + dec_cols):
    if col not in df.columns:
        df[col] = 0

# ---------- æ»‘çª— - åŒè¾“å‡º ----------
def make_ds(data):
    Xp,Xf,Y_energy,Y_power = [],[],[],[]
    
    # æœ€ç»ˆæ•°æ®éªŒè¯
    print("INFO: Validating data quality...")
    for col in set(enc_cols + dec_cols + ['load_discharge_delta', 'total_active_power']):
        if col in data.columns:
            if data[col].isnull().any():
                print(f"WARNING: {col} has {data[col].isnull().sum()} null values, filling with median")
                data[col] = data[col].fillna(data[col].median())
            if np.isinf(data[col]).any():
                print(f"WARNING: {col} has infinite values, replacing with boundary values")
                data[col] = data[col].replace([np.inf, -np.inf], [data[col].quantile(0.99), data[col].quantile(0.01)])
    
    enc = sc_enc.transform(data[enc_cols])
    dec = sc_dec.transform(data[dec_cols])
    y_energy = sc_y_energy.transform(data[['load_discharge_delta']])
    y_power  = sc_y_power.transform(data[['total_active_power']])
    for i in range(len(data)-PAST-FUT+1):
        Xp.append(enc[i:i+PAST])
        Xf.append(dec[i+PAST:i+PAST+FUT])
        Y_energy.append(y_energy[i+PAST:i+PAST+FUT,0])
        Y_power.append(y_power[i+PAST:i+PAST+FUT,0])
    return (np.array(Xp,np.float32), np.array(Xf,np.float32), 
            np.array(Y_energy,np.float32), np.array(Y_power,np.float32))

Xp,Xf,Y_energy,Y_power = make_ds(df)
if len(Xp)==0:
    raise ValueError('å¢é‡æ•°æ®ä¸è¶³ä»¥å½¢æˆæ»‘çª—æ ·æœ¬')
loader = DataLoader(TensorDataset(torch.from_numpy(Xp),
                                  torch.from_numpy(Xf),
                                  torch.from_numpy(Y_energy),
                                  torch.from_numpy(Y_power)),
                    batch_size=BATCH_SIZE, shuffle=True)

# ---------- æ¨¡å‹ - åŒè¾“å‡º ----------
class EncDec(nn.Module):
    def __init__(self,d_enc,d_dec,hid,drop):
        super().__init__()
        self.enc = nn.LSTM(d_enc,hid,batch_first=True)
        self.dec = nn.LSTM(d_dec,hid,batch_first=True)
        self.dp  = nn.Dropout(drop)
        self.fc_energy = nn.Linear(hid,1)  # ç”µé‡é¢„æµ‹
        self.fc_power  = nn.Linear(hid,1)  # åŠŸç‡é¢„æµ‹
    def forward(self,xe,xd):
        _,(h,c)=self.enc(xe)
        out,_ = self.dec(xd,(h,c))
        out_dp = self.dp(out)
        energy_pred = self.fc_energy(out_dp).squeeze(-1)
        power_pred = self.fc_power(out_dp).squeeze(-1)
        return energy_pred, power_pred

# ---------- å¢å¼ºæ¨¡å‹ç»“æ„ï¼ˆä¸fine_tune_station.pyä¸€è‡´ï¼‰ - åŒè¾“å‡º ----------
class EnhancedEncDec(nn.Module):
    def __init__(self, base_model, hid, drop):
        super().__init__()
        # å¤åˆ¶åŸºç¡€æ¨¡å‹çš„å‚æ•°
        self.enc = base_model.enc
        self.dec = base_model.dec
        self.dp = base_model.dp
        
        # æ·»åŠ å¢å¼ºå±‚
        self.bn = nn.BatchNorm1d(hid)
        # ä½¿ç”¨æ›´å¤æ‚çš„è¾“å‡ºå±‚
        self.fc_energy_enhanced = nn.Sequential(
            nn.Linear(hid, hid//2),
            nn.ReLU(),
            nn.Dropout(drop/2),
            nn.Linear(hid//2, 1)
        )
        self.fc_power_enhanced = nn.Sequential(
            nn.Linear(hid, hid//2),
            nn.ReLU(),
            nn.Dropout(drop/2),
            nn.Linear(hid//2, 1)
        )
        
        # ä¿ç•™åŸå§‹è¾“å‡ºå±‚ç”¨äºæ®‹å·®è¿æ¥
        self.fc_energy_orig = base_model.fc_energy
        self.fc_power_orig = base_model.fc_power
        
    def forward(self, xe, xd):
        _, (h,c) = self.enc(xe)
        out,_ = self.dec(xd,(h,c))
        out_dp = self.dp(out)
        
        # åº”ç”¨æ‰¹å½’ä¸€åŒ–ï¼ˆéœ€è¦è°ƒæ•´ç»´åº¦ï¼‰
        batch_size, seq_len, hidden_size = out_dp.shape
        out_bn = self.bn(out_dp.transpose(1, 2)).transpose(1, 2)
        
        # å¢å¼ºé¢„æµ‹ + åŸå§‹é¢„æµ‹çš„æ®‹å·®è¿æ¥
        energy_enhanced = self.fc_energy_enhanced(out_bn).squeeze(-1)
        power_enhanced = self.fc_power_enhanced(out_bn).squeeze(-1)
        
        energy_orig = self.fc_energy_orig(out_dp).squeeze(-1)
        power_orig = self.fc_power_orig(out_dp).squeeze(-1)
        
        # æ®‹å·®è¿æ¥ï¼šå¢å¼ºé¢„æµ‹ + 0.3 * åŸå§‹é¢„æµ‹
        energy_pred = energy_enhanced + 0.3 * energy_orig
        power_pred = power_enhanced + 0.3 * power_orig
        
        return energy_pred, power_pred

# --- Weighted L1 (é’ˆå¯¹å•ç«™ä¼˜åŒ–æƒé‡) ---
class WeightedL1(nn.Module):
    def __init__(self,fut,device):
        super().__init__()
        # æ ¹æ®MAPEç»“æœè°ƒæ•´æƒé‡ï¼šå¯¹è¡¨ç°å·®çš„å¤©æ•°ç»™æ›´é«˜æƒé‡
        w = np.concatenate([
            np.ones(96)*1.8,      # Day1: 59.68% -> æé«˜æƒé‡
            np.ones(96)*2.2,      # Day2: 89.27% -> æœ€é«˜æƒé‡
            np.ones(96)*1.0,      # Day3: 25.58% -> ä¿æŒåŸºç¡€æƒé‡
            np.ones(96)*1.6,      # Day4: 52.51% -> æé«˜æƒé‡
            np.ones(96)*2.0,      # Day5: 92.49% -> é«˜æƒé‡
            np.ones(96)*1.0,      # Day6: 16.92% -> ä¿æŒåŸºç¡€æƒé‡
            np.ones(96)*1.7       # Day7: 64.59% -> æé«˜æƒé‡
        ])
        self.register_buffer('w', torch.tensor(w,dtype=torch.float32,device=device))
    def forward(self,pred,tgt):
        return torch.mean(self.w*torch.abs(pred-tgt))

# åˆ›å»ºåŸºç¡€æ¨¡å‹
base_model = EncDec(len(enc_cols), len(dec_cols), 128, .24).to(DEVICE)

# --- è½½å…¥å†å²æƒé‡ ---
opt_path = f'{MODEL_DIR}/model_optimized_{STATION_ID}.pth'
base_path= f'{ROOT}/model_weighted.pth'   # ä½ çš„æ€»æ¨¡å‹æƒé‡

if os.path.exists(opt_path):
    # å¦‚æœå­˜åœ¨å¾®è°ƒæƒé‡ï¼Œä½¿ç”¨å¢å¼ºæ¨¡å‹
    base_model.load_state_dict(torch.load(base_path, map_location=DEVICE))
    model = EnhancedEncDec(base_model, 128, .24).to(DEVICE)
    model.load_state_dict(torch.load(opt_path, map_location=DEVICE))
    print(">>> åŠ è½½å·²å¾®è°ƒçš„å¢å¼ºæ¨¡å‹æƒé‡")
else:
    # å¦‚æœä¸å­˜åœ¨å¾®è°ƒæƒé‡ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹
    model = base_model
    model.load_state_dict(torch.load(base_path, map_location=DEVICE))
    print(">>> åŠ è½½åŸºç¡€æƒé‡ï¼ˆé¦–æ¬¡å¢é‡ï¼‰")

# å†»ç»“ Encoder
for p in model.enc.parameters():
    p.requires_grad_(False)

criterion = WeightedL1(FUT, DEVICE)
# ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ·»åŠ æƒé‡è¡°å‡
optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, model.parameters()), 
                             lr=LR_INCR, weight_decay=1e-5)
# è°ƒæ•´è°ƒåº¦å™¨å‚æ•°ï¼Œæ›´è€å¿ƒç­‰å¾…
scheduler = ReduceLROnPlateau(optimizer,'min',patience=5,factor=.7,min_lr=1e-6)

best=float('inf'); wait=0
print(f"ğŸš€ å¢é‡è®­ç»ƒæ ·æœ¬ {len(loader.dataset)}   batch {BATCH_SIZE}")
for ep in range(1,EPOCHS+1):
    model.train(); ep_loss=0
    for xe,xd,yy_energy,yy_power in loader:
        xe,xd,yy_energy,yy_power = xe.to(DEVICE),xd.to(DEVICE),yy_energy.to(DEVICE),yy_power.to(DEVICE)
        optimizer.zero_grad()
        pred_energy, pred_power = model(xe,xd)
        loss_energy = criterion(pred_energy,yy_energy)
        loss_power = criterion(pred_power,yy_power)
        loss = loss_energy + loss_power
        loss.backward(); optimizer.step()
        ep_loss += loss.item()
    ep_loss /= len(loader); scheduler.step(ep_loss)

    log=f'E{ep:02d} loss {ep_loss:.4f}'
    if ep_loss<best:
        best=ep_loss; wait=0
        torch.save(model.state_dict(), opt_path)
        log+=' âœ” save'
    else:
        wait+=1
        if wait>=8:  # å¢åŠ æ—©åœè€å¿ƒï¼Œç»™æ¨¡å‹æ›´å¤šæ—¶é—´ä¼˜åŒ–
            log+=' (early stop)'; print(log); break
    print(log)

# ---------- ç®€æ˜“è¯„ä¼° - åŒè¾“å‡º ----------
def day_mape(t,p):
    res=[]
    for d in range(7):
        s,e=d*96,(d+1)*96
        t0,t1=t[s:e],p[s:e]
        # æ›´ä¸¥æ ¼çš„å¤„ç†ï¼šè¿‡æ»¤æ‰å¼‚å¸¸å€¼
        mask = (np.abs(t0) > 1e-3) & np.isfinite(t0) & np.isfinite(t1)
        if mask.sum() == 0:
            res.append(0.0)  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›0
        else:
            t0_filtered = t0[mask]
            t1_filtered = t1[mask]
            # ä½¿ç”¨ç»å¯¹å€¼ç¡®ä¿åˆ†æ¯ä¸ºæ­£
            t0_filtered = np.where(np.abs(t0_filtered) < 1e-3, 
                                 np.sign(t0_filtered) * 1e-3, t0_filtered)
            mape = np.mean(np.abs((t0_filtered - t1_filtered) / t0_filtered)) * 100
            # é™åˆ¶MAPEçš„æœ€å¤§å€¼ï¼Œé¿å…æç«¯æƒ…å†µ
            res.append(min(mape, 1000.0))
    return res

model.eval()
with torch.no_grad():
    xe=torch.from_numpy(Xp[-1:].astype(np.float32)).to(DEVICE)
    xd=torch.from_numpy(Xf[-1:].astype(np.float32)).to(DEVICE)
    pred_energy_s, pred_power_s = model(xe,xd)
    pred_energy_s = pred_energy_s.cpu().numpy().flatten()
    pred_power_s = pred_power_s.cpu().numpy().flatten()
    
    pred_energy = sc_y_energy.inverse_transform(pred_energy_s.reshape(-1,1)).flatten()
    pred_power = sc_y_power.inverse_transform(pred_power_s.reshape(-1,1)).flatten()
    
    true_energy = sc_y_energy.inverse_transform(Y_energy[-1:].reshape(-1,1)).flatten()
    true_power = sc_y_power.inverse_transform(Y_power[-1:].reshape(-1,1)).flatten()

    # å®‰å…¨çš„MAPEè®¡ç®—
    def safe_mape(y_true, y_pred):
        mask = (np.abs(y_true) > 1e-3) & np.isfinite(y_true) & np.isfinite(y_pred)
        if mask.sum() == 0:
            return 0.0
        y_true_filtered = y_true[mask]
        y_pred_filtered = y_pred[mask]
        y_true_filtered = np.where(np.abs(y_true_filtered) < 1e-3, 
                                 np.sign(y_true_filtered) * 1e-3, y_true_filtered)
        mape = np.mean(np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)) * 100
        return min(mape, 1000.0)
    
    overall_energy = safe_mape(true_energy, pred_energy)
    overall_power = safe_mape(true_power, pred_power)
    
    dm_energy = day_mape(true_energy,pred_energy)
    dm_power = day_mape(true_power,pred_power)

print(f'\nRESULT: Energy 7-day MAPE {overall_energy:.2f}%  | '+
      '  '.join([f'D{i+1}:{m:.2f}%' for i,m in enumerate(dm_energy)]))
print(f'RESULT: Power  7-day MAPE {overall_power:.2f}%   | '+
      '  '.join([f'D{i+1}:{m:.2f}%' for i,m in enumerate(dm_power)]))

# ---------- æ›´æ–°å°¾å·´ç¼“å­˜ ----------
# ä¿å­˜æ—¶ä½¿ç”¨åŸå§‹å­—æ®µå ts
df_cache = df.tail(PAST+FUT).copy()
df_cache = df_cache.rename(columns={'energy_date': 'ts'})
df_cache.to_csv(TAIL_PATH,index=False)
print("\nSUCCESS: Incremental fine-tuning completed, weights & tail cache updated")
